{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5bdaf4",
   "metadata": {},
   "source": [
    "#   Torch IO library for 3-D images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb39d128",
   "metadata": {},
   "source": [
    "##   TorchIO Image class\n",
    "\n",
    "> The Image class, representing one medical image, stores a 4D tensor, whose voxels encode, e.g., signal intensity or segmentation labels, and the corresponding affine transform, typically a rigid (Euclidean) transform, to convert voxel indices to world coordinates in mm. Arbitrary fields such as acquisition parameters may also be stored.\n",
    "\n",
    "> Subclasses are used to indicate specific types of images, such as ScalarImage and LabelMap, which are used to store, e.g., CT scans and segmentations, respectively.\n",
    "\n",
    "> An instance of Image can be created using a filepath, a PyTorch tensor, or a NumPy array. This class uses lazy loading, i.e., the data is not loaded from disk at instantiation time. Instead, the data is only loaded when needed for an operation (e.g., if a transform is applied to the image).\n",
    "\n",
    "> The figure below shows two instances of Image. The instance of ScalarImage contains a 4D tensor representing a diffusion MRI, which contains four 3D volumes (one per gradient direction), and the associated affine matrix. Additionally, it stores the strength and direction for each of the four gradients. The instance of LabelMap contains a brain parcellation of the same subject, the associated affine matrix, and the name and color of each brain structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410fd988",
   "metadata": {},
   "source": [
    "<img src='https://torchio.readthedocs.io/_images/data_structures.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e16c1101",
   "metadata": {},
   "source": [
    "##   class torchio.ScalarImage(*args, **kwargs)[source] =>  Image whose pixel values represent scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f730b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "# Loading from a file\n",
    "t1_image = tio.ScalarImage('t1.nii.gz')\n",
    "dmri = tio.ScalarImage(tensor=torch.rand(32, 128, 128, 88))\n",
    "image = tio.ScalarImage('safe_image.nrrd', check_nans=False)\n",
    "data, affine = image.data, image.affine\n",
    "\n",
    "print (affine.shape)\n",
    "print (image.data is image[tio.DATA])\n",
    "print (image.data is image.tensor )\n",
    "print (type(image.data) )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3817ee72",
   "metadata": {},
   "source": [
    "##   class torchio.LabelMap(*args, **kwargs)[source]  =>  Image whose pixel values represent categorical labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f39bbe",
   "metadata": {},
   "source": [
    "> Intensity transforms are not applied to these images.\n",
    "\n",
    "> Nearest neighbor interpolation is always used to resample label maps, independently of the specified interpolation type in the transform instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "labels = tio.LabelMap(tensor=torch.rand(1, 128, 128, 68) > 0.5)\n",
    "labels = tio.LabelMap('t1_seg.nii.gz')  \n",
    "tpm = tio.LabelMap('gray_matter.nii.gz', 'white_matter.nii.gz', 'csf.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15a116",
   "metadata": {},
   "source": [
    "<img src='images/image_class.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8536c",
   "metadata": {},
   "source": [
    "<img src='images/parameters.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d69ee453",
   "metadata": {},
   "source": [
    "##  TorchIO images are lazy loaders, i.e. the data is only loaded from disk when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "import numpy as np\n",
    "\n",
    "image = tio.ScalarImage('t1.nii.gz')  # subclass of Image\n",
    "image  # not loaded yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_two = 2 * image.data  # data is loaded and cached here\n",
    "print (image)\n",
    "\n",
    "image.save('doubled_image.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf231cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function\n",
    "def numpy_reader(path):\n",
    "    data = np.load(path).as_type(np.float32)\n",
    "    affine = np.eye(4)\n",
    "    return data, affine\n",
    "\n",
    "image = tio.ScalarImage('t1.npy', reader=numpy_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642aef37",
   "metadata": {},
   "outputs": [],
   "source": [
    " => affine: numpy.ndarray => Affine matrix to transform voxel indices into world coordinates.\n",
    "        \n",
    " => as_pil(transpose=True) => Get the image as an instance of PIL.Image.\n",
    "\n",
    "     ** Note => Values will be clamped to 0-255 and cast to uint8 and \n",
    "                    To use this method, Pillow needs to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    " => as_sitk(**kwargs) → SimpleITK.SimpleITK.Image  => Get the image as an instance of sitk.Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9d494",
   "metadata": {},
   "outputs": [],
   "source": [
    " => axis_name_to_index(axis: str) => int  =>  Convert an axis name to an axis index.\n",
    "    \n",
    "   ** axis – Possible inputs are 'Left', 'Right', 'Anterior', 'Posterior', 'Inferior', 'Superior'. \n",
    "      \n",
    "   ** Lower-case versions and first letters are also valid, as only the first letter will be used.\n",
    "\n",
    " => If you are working with animals, you should probably use 'Superior', 'Inferior', 'Anterior' and 'Posterior'\n",
    "     for 'Dorsal', 'Ventral', 'Rostral' and 'Caudal', respectively.\n",
    "    \n",
    " => If your images are 2D, you can use 'Top', 'Bottom', 'Left' and 'Right'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8de967",
   "metadata": {},
   "outputs": [],
   "source": [
    " => bounds: numpy.ndarray  =>  Position of centers of voxels in smallest and largest indices.\n",
    "\n",
    " => data: torch.Tensor  =>  Tensor data (Same as Image.tensor)\n",
    "    \n",
    " => static flip_axis(axis: str) → str[source]  => Return the opposite axis label. For example, 'L' -> 'R'.\n",
    "\n",
    "  ** axis – Axis label, such as 'L' or 'left'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c93ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  classmethod from_sitk(sitk_image)  =>  Instantiate a new TorchIO image from a sitk.Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dca343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "import SimpleITK as sitk\n",
    "\n",
    "sitk_image = sitk.Image(20, 30, 40, sitk.sitkUInt16)\n",
    "tio.LabelMap.from_sitk(sitk_image)\n",
    "sitk_image = sitk.Image((224, 224), sitk.sitkVectorFloat32, 3)\n",
    "tio.ScalarImage.from_sitk(sitk_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bcfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  get_bounds() => Tuple[Tuple[float, float], Tuple[float, float], Tuple[float, float]]\n",
    "                          Get minimum and maximum world coordinates occupied by the image.\n",
    "        \n",
    " =>  get_center(lps: bool = False) => Tuple[float, float, float] =>  Get image center in RAS+ or LPS+ coordinates.\n",
    "\n",
    "    ** lps – If True, the coordinates will be in LPS+ orientation, i.e. the first dimension grows towards\n",
    "               the left etc. Otherwise, the coordinates will be in RAS+ orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  height: int  =>  Image height, if 2D.\n",
    "        \n",
    " =>  itemsize =>  Element size of the data type.\n",
    "\n",
    " =>  load() => None => Load the image from disk.\n",
    "    \n",
    "     Returns => Tuple containing a 4D tensor of size (C,W,H,D) and a 2D(4,4)  affine matrix to convert \n",
    "                 voxel indices to world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a93b96",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  memory: float  =>  Number of Bytes that the tensor takes in the RAM.\n",
    "        \n",
    " =>  num_channels: int  => Get the number of channels in the associated 4D tensor.\n",
    "\n",
    " =>  numpy() → numpy.ndarray  =>  Get a NumPy array containing the image data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f39491",
   "metadata": {},
   "outputs": [],
   "source": [
    " => orientation: Tuple[str, str, str]  =>  Orientation codes.\n",
    "        \n",
    " => origin: Tuple[float, float, float]  => Center of first voxel in array, in mm.\n",
    "\n",
    " => plot(**kwargs) → None =>  Plot image.\n",
    "\n",
    " => save(path: Union[str, os.PathLike], squeeze: Optional[bool] = None) => None  =>  Save image to disk.\n",
    "\n",
    "      ** path – String or instance of pathlib.Path.\n",
    "\n",
    "      ** squeeze – Whether to remove singleton dimensions before saving. \n",
    "                     If None, the array will be squeezed if the output format is JP(E)G, PNG, BMP or TIF(F).\n",
    "\n",
    "\n",
    " => set_data(tensor: Union[torch.Tensor, numpy.ndarray]) =>  Store a 4D tensor in the data key and attribute.\n",
    "\n",
    "     ** tensor – 4D tensor (C,W,H,D) with dimensions .\n",
    "\n",
    " => shape: Tuple[int, int, int, int] => Tensor shape as (C,W,H,D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    " => show(viewer_path: Optional[Union[str, os.PathLike]] = None) => None => Open the image using external software.\n",
    "    \n",
    "    **  viewer_path – Path to the application used to view the image. \n",
    "                      If None, the value of the environment variable SITK_SHOW_COMMAND will be used. \n",
    "                      If this variable is also not set, TorchIO will try to guess the location of ITK-SNAP \n",
    "                         and 3D Slicer.  RAISES RuntimeError – If the viewer is not found.\n",
    " => spacing: Tuple[float, float, float] => Voxel spacing in mm.\n",
    "    \n",
    " => spatial_shape: Tuple[int, int, int]  =>  Tensor spatial shape as (C,W,H,D) .\n",
    "    \n",
    " => tensor: torch.Tensor =>  Tensor data (Same as Image.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe53064",
   "metadata": {},
   "outputs": [],
   "source": [
    " => to_gif(axis: int, duration: float, output_path: Union[str, os.PathLike], loop: int = 0,\n",
    "            rescale: bool = True, optimize: bool = True, reverse: bool = False) => None\n",
    "    \n",
    " => to_gif() saves an animated GIF of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bacd3a",
   "metadata": {},
   "source": [
    "<img src=\"images/to_gif.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28073d54",
   "metadata": {},
   "source": [
    " ##  Subject Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98d14f",
   "metadata": {},
   "source": [
    "<img src='images/subject_class.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ee456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "\n",
    "# One way:\n",
    "subject = tio.Subject(one_image=tio.ScalarImage('path_to_image.nii.gz'), \n",
    "                      a_segmentation=tio.LabelMap('path_to_seg.nii.gz'),\n",
    "                      age=45, name='John Doe', hospital='Hospital Juan Negrín')\n",
    "\n",
    "# If you want to create the mapping before, or have spaces in the keys -\n",
    "subject_dict = {'one image': tio.ScalarImage('path_to_image.nii.gz'),\n",
    "                'a segmentation': tio.LabelMap('path_to_seg.nii.gz'),\n",
    "                'age': 45, 'name': 'John Doe', 'hospital': 'Hospital Juan Negrín'}\n",
    "\n",
    "subject = tio.Subject(subject_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ca634",
   "metadata": {},
   "outputs": [],
   "source": [
    " => add_image(image: torchio.data.image.Image, image_name: str) => None  =>  Add an image.\n",
    "\n",
    " => apply_inverse_transform(**kwargs) → torchio.data.subject.Subject => Try to apply the inverse of all \n",
    "                                                                          applied transforms, in reverse order.\n",
    "\n",
    "       **kwargs – Keyword arguments passed on to get_inverse_transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc46c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##   Check for consistency of an attribute across all images.\n",
    "\n",
    "=> check_consistent_attribute(attribute: str, relative_tolerance: float = 1e-06, \n",
    "                               absolute_tolerance: float = 1e-06, message: Optional[str] = None) → None\n",
    "    \n",
    "  ## Parameters  -\n",
    "   \n",
    "   * attribute – Name of the image attribute to check\n",
    "\n",
    "   * relative_tolerance – Relative tolerance for numpy.allclose()\n",
    "\n",
    "   * absolute_tolerance – Absolute tolerance for numpy.allclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "scalars = torch.randn(1, 512, 512, 100)\n",
    "mask = torch.tensor(scalars > 0).type(torch.int16)\n",
    "\n",
    "af1 = np.eye([0.8, 0.8, 2.50000000000001, 1])\n",
    "af2 = np.eye([0.8, 0.8, 2.49999999999999, 1])  # small difference here (e.g. due to different reader)\n",
    "\n",
    "subject = tio.Subject(image = tio.ScalarImage(tensor=scalars, affine=af1),\n",
    "                      mask = tio.LabelMap(tensor=mask, affine=af2))\n",
    "\n",
    "subject.check_consistent_attribute('spacing')  # no error as tolerances are > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e8d4b",
   "metadata": {},
   "source": [
    "<img src='images/relative_tolerance.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a27ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Get a reversed list of the inverses of the applied transforms.\n",
    "    \n",
    "=> get_inverse_transform(warn: bool = True, ignore_intensity: bool = True, image_interpolation: \n",
    "                           Optional[str] = None) → Compose \n",
    "    \n",
    " \n",
    "          Parameters \n",
    "    \n",
    "    * warn – Issue a warning if some transforms are not invertible.\n",
    "\n",
    "    * ignore_intensity – If True, all instances of IntensityTransform will be ignored.\n",
    "\n",
    "    * image_interpolation – Modify interpolation for scalar images inside transforms that perform resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ab17e",
   "metadata": {},
   "outputs": [],
   "source": [
    " => load() → None  =>  Load images in subject on RAM.\n",
    "    \n",
    " => plot(**kwargs) → None  =>  Plot images using matplotlib.\n",
    "\n",
    "        **kwargs – Keyword arguments that will be passed on to plot()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bfb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    " => remove_image(image_name: str) => None  =>  Remove an image.\n",
    "    \n",
    " => shape => Return shape of first image in subject.\n",
    "\n",
    " ** Note => Consistency of shapes across images in the subject is checked first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f68ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "\n",
    "colin = tio.datasets.Colin27()\n",
    "colin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923280f1",
   "metadata": {},
   "outputs": [],
   "source": [
    " => spacing => Return spacing of first image in subject.\n",
    "\n",
    "  ** Note =>  Consistency of spacings across images in the subject is checked first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9286c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "\n",
    "colin = tio.datasets.Slicer()\n",
    "colin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ce594",
   "metadata": {},
   "outputs": [],
   "source": [
    " => spatial_shape  =>  Return spatial shape of first image in subject.\n",
    "\n",
    "  ** Note =>  Consistency of spatial shapes across images in the subject is checked first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f241c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "\n",
    "colin = tio.datasets.Colin27()\n",
    "colin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb62a54",
   "metadata": {},
   "source": [
    "##  DATASET Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842c317",
   "metadata": {},
   "source": [
    "<img src='https://torchio.readthedocs.io/_images/diagram_volumes.svg' />\n",
    "</br>\n",
    "<img src='images/dataset.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "subject_a = tio.Subject(\n",
    "    t1=tio.ScalarImage('t1.nrrd',),\n",
    "    t2=tio.ScalarImage('t2.mha',),\n",
    "    label=tio.LabelMap('t1_seg.nii.gz'),\n",
    "    age=31,\n",
    "    name='Fernando Perez',\n",
    ")\n",
    "subject_b = tio.Subject(\n",
    "    t1=tio.ScalarImage('colin27_t1_tal_lin.minc',),\n",
    "    t2=tio.ScalarImage('colin27_t2_tal_lin_dicom',),\n",
    "    label=tio.LabelMap('colin27_seg1.nii.gz'),\n",
    "    age=56,\n",
    "    name='Colin Holmes',\n",
    ")\n",
    "subjects_list = [subject_a, subject_b]\n",
    "transforms = [\n",
    "    tio.RescaleIntensity(out_min_max=(0, 1)),\n",
    "    tio.RandomAffine(),\n",
    "]\n",
    "transform = tio.Compose(transforms)\n",
    "subjects_dataset = tio.SubjectsDataset(subjects_list, transform=transform)\n",
    "subject = subjects_dataset[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ed97d",
   "metadata": {},
   "outputs": [],
   "source": [
    " => To quickly iterate over the subjects without loading the images, use dry_iter().\n",
    "\n",
    " =>  dry_iter()  =>  Return the internal list of subjects.\n",
    "\n",
    " => This can be used to iterate over the subjects without loading the data and applying any transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [subject.name for subject in dataset.dry_iter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637fde5d",
   "metadata": {},
   "outputs": [],
   "source": [
    " => classmethod from_batch(batch: Dict) → torchio.data.dataset.SubjectsDataset\n",
    "    \n",
    " => Instantiates a dataset from a batch generated by a data loader.\n",
    "\n",
    "    ** batch – Dictionary generated by a data loader, containing data that can be converted to \n",
    "                  instances of Subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03977647",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  set_transform(transform: Optional[Callable]) =>  None  =>  Set the transform attribute\n",
    "    \n",
    "    ** transform – Callable object, typically an subclass of torchio.transforms.Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3442082",
   "metadata": {},
   "source": [
    "  ##  Patch-based Pipelines for Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    " => The number of pixels in 2D images used in deep learning is rarely larger than one million.\n",
    "      For example, the input size of several popular image classification models is 224x224x3 = 150x528 pixels \n",
    "        (588 KiB if 32 bits per pixel are used). \n",
    "        \n",
    " => In contrast, 3D medical images often contain hundreds of millions of voxels, and downsampling might not be \n",
    "     acceptable when small details should be preserved. For example, the size of a high-resolution lung CT-scan\n",
    "      used for quantifying chronic obstructive pulmonary disease damage in a research setting,\n",
    "        with spacing 0.66  0.66  0.30 mm, is 512  512  1069 = 280 231 936 voxels\n",
    "             (1.04 GiB if 32 bits per voxel are used).\n",
    "\n",
    " => In computer vision applications, images used for training are grouped in batches whose size is often in the\n",
    "     order of hundreds or even thousands of training instances, depending on the available GPU memory. \n",
    " \n",
    " => In medical image applications, batches rarely contain more than one or two training instances due to their\n",
    "      larger memory footprint compared to natural images. \n",
    "    \n",
    " => This reduces the utility of techniques such as batch normalization, which rely on batches being large enough\n",
    "      to estimate dataset variance appropriately. Moreover, large image size and small batches result in longer training time, hindering the experimental cycle \n",
    "       that is necessary for hyperparameter optimization.\n",
    "    \n",
    " => In cases where GPU memory is limited and the network architecture is large, it is possible that not even the\n",
    "      entirety of a single volume can be processed during a single training iteration.\n",
    "        \n",
    " => To overcome this challenge, it is common in medical imaging to train using subsets of the image, \n",
    "      or image patches, randomly extracted from the volumes.\n",
    "\n",
    " => Networks can be trained with 2D slices extracted from 3D volumes, aggregating the inference results to generate \n",
    "      a 3D volume. This can be seen as a specific case of patch-based training, where the size of the patches \n",
    "       along a dimension is one. \n",
    "        \n",
    "    Other methods extract volumetric patches for training, that are often cubes, if the voxel spacing is isotropic,\n",
    "     or cuboids adapted to the anisotropic spacing of the training images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be082cfd",
   "metadata": {},
   "source": [
    "<img src='images/uniform-sampler.png'>\n",
    "<img src='images/weighted-sampler.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855395b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "subject = tio.Subject(\n",
    "    t1=tio.ScalarImage('t1_mri.nii.gz'),\n",
    "    sampling_map=tio.Image('sampling.nii.gz', type=tio.SAMPLING_MAP),\n",
    ")\n",
    "patch_size = 64\n",
    "sampler = tio.data.WeightedSampler(patch_size, 'sampling_map')\n",
    "for patch in sampler(subject):\n",
    "    print(patch[tio.LOCATION])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8583c005",
   "metadata": {},
   "source": [
    "<img src='images/note1.png'>\n",
    "<img src='images/label-sampler.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d53d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "subject = tio.datasets.Colin27()\n",
    "subject\n",
    "probabilities = {0: 0.5, 1: 0.5}\n",
    "sampler = tio.data.LabelSampler(\n",
    "    patch_size=64,\n",
    "    label_name='brain',\n",
    "    label_probabilities=probabilities,\n",
    ")\n",
    "generator = sampler(subject)\n",
    "for patch in generator:\n",
    "    print(patch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  If you want a specific number of patches from a volume, e.g. 10 .\n",
    "\n",
    "generator = sampler(subject, num_patches=10)\n",
    "for patch in iterator:\n",
    "    print(patch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292bbb7",
   "metadata": {},
   "source": [
    "<img src='images/patchsampler.png'>\n",
    "<img src='images/gridsampler.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f78603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "sampler = tio.GridSampler(patch_size=88)\n",
    "colin = tio.datasets.Colin27()\n",
    "for i, patch in enumerate(sampler(colin)):\n",
    "    patch.t1.save(f'patch_{i}.nii.gz')\n",
    "# To figure out the number of patches beforehand:\n",
    "sampler = tio.GridSampler(subject=colin, patch_size=88)\n",
    "len(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644a505",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/fepegar/torchio/main/docs/images/diagram_patches.svg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    " => This sketch can be used to experiment and understand how the queue works. \n",
    "      In this case, shuffle_subjects is False and shuffle_patches is True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023df4f",
   "metadata": {},
   "source": [
    "<img src='images/download.png'>\n",
    "<img src='images/download1.png'>\n",
    "<img src='images/download2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    " ** Note =>  num_workers refers to the number of workers used to load and transform the volumes. \n",
    "               Multiprocessing is not needed to pop patches from the queue, so you should always use num_workers=0\n",
    "                 for the DataLoader you instantiate to generate training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader\n",
    "patch_size = 96\n",
    "queue_length = 300\n",
    "samples_per_volume = 10\n",
    "sampler = tio.data.UniformSampler(patch_size)\n",
    "subject = tio.datasets.Colin27()\n",
    "subjects_dataset = tio.SubjectsDataset(10 * [subject])\n",
    "patches_queue = tio.Queue(\n",
    "    subjects_dataset,\n",
    "    queue_length,\n",
    "    samples_per_volume,\n",
    "    sampler,\n",
    "    num_workers=4,\n",
    ")\n",
    "patches_loader = DataLoader(\n",
    "    patches_queue,\n",
    "    batch_size=16,\n",
    "    num_workers=0,  # this must be 0\n",
    ")\n",
    "num_epochs = 2\n",
    "model = torch.nn.Identity()\n",
    "for epoch_index in range(num_epochs):\n",
    "    for patches_batch in patches_loader:\n",
    "        inputs = patches_batch['t1'][tio.DATA]  # key 't1' is in subject\n",
    "        targets = patches_batch['brain'][tio.DATA]  # key 'brain' is in subject\n",
    "        logits = model(inputs)  # model being an instance of torch.nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def148c1",
   "metadata": {},
   "source": [
    "<img src='images/max-memory.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7461327f",
   "metadata": {},
   "source": [
    "##  Inference\n",
    "\n",
    "> Here is an example that uses a grid sampler and aggregator to perform dense inference across a 3D image using patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c633c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "patch_overlap = 4, 4, 4  # or just 4\n",
    "patch_size = 88, 88, 60\n",
    "subject = tio.datasets.Colin27()\n",
    "subject\n",
    "grid_sampler = tio.inference.GridSampler(\n",
    "    subject,\n",
    "    patch_size,\n",
    "    patch_overlap,\n",
    ")\n",
    "patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=4)\n",
    "aggregator = tio.inference.GridAggregator(grid_sampler)\n",
    "model = nn.Identity().eval()\n",
    "with torch.no_grad():\n",
    "    for patches_batch in patch_loader:\n",
    "        input_tensor = patches_batch['t1'][tio.DATA]\n",
    "        locations = patches_batch[tio.LOCATION]\n",
    "        logits = model(input_tensor)\n",
    "        labels = logits.argmax(dim=tio.CHANNELS_DIMENSION, keepdim=True)\n",
    "        outputs = labels\n",
    "        aggregator.add_batch(outputs, locations)\n",
    "output_tensor = aggregator.get_output_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebca5f",
   "metadata": {},
   "source": [
    "<img src='images/gridsampler1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbed069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "sampler = tio.GridSampler(patch_size=88)\n",
    "colin = tio.datasets.Colin27()\n",
    "for i, patch in enumerate(sampler(colin)):\n",
    "    patch.t1.save(f'patch_{i}.nii.gz')\n",
    "# To figure out the number of patches beforehand:\n",
    "sampler = tio.GridSampler(subject=colin, patch_size=88)\n",
    "len(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ae84c",
   "metadata": {},
   "source": [
    "<img src='images/grid1.png'>\n",
    "<img src='images/grid2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc8888f",
   "metadata": {},
   "source": [
    "##  Transforms \n",
    "\n",
    "<img src='https://torchio.readthedocs.io/_images/fpg_progressive.gif'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05676c81",
   "metadata": {},
   "outputs": [],
   "source": [
    " => TorchIO transforms take as input instances of Subject or Image (and its subclasses), 4D PyTorch tensors,\n",
    "      4D NumPy arrays, SimpleITK images, NiBabel images, or Python dictionaries (see Transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "affine_transform = tio.RandomAffine()\n",
    "tensor = torch.rand(1, 256, 256, 159)\n",
    "transformed_tensor = affine_transform(tensor)\n",
    "type(transformed_tensor)\n",
    "array = np.random.rand(1, 256, 256, 159)\n",
    "transformed_array = affine_transform(array)\n",
    "type(transformed_array)\n",
    "subject = tio.datasets.Colin27()\n",
    "transformed_subject = affine_transform(subject)\n",
    "transformed_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4289376",
   "metadata": {},
   "outputs": [],
   "source": [
    " => Transforms can also be applied from the command line using torchio-transform. All transforms inherit\n",
    "     from torchio.transforms.Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfc79c",
   "metadata": {},
   "source": [
    "<img src='images/transform.png'>\n",
    "<img src='images/call.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb328f2",
   "metadata": {},
   "source": [
    "<img src='images/transformability.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "spatial_transforms = {\n",
    "    tio.RandomElasticDeformation(): 0.2,\n",
    "    tio.RandomAffine(): 0.8,\n",
    "}\n",
    "transform = tio.Compose([\n",
    "    tio.OneOf(spatial_transforms, p=0.5),\n",
    "    tio.RescaleIntensity(out_min_max=(0, 1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37620864",
   "metadata": {},
   "source": [
    "<img src='images/reproducibility.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58433c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "subject = tio.datasets.FPG()\n",
    "transforms = (\n",
    "    tio.CropOrPad((100, 200, 300)),\n",
    "    tio.RandomFlip(axes=['LR', 'AP', 'IS']),\n",
    "    tio.OneOf([tio.RandomAnisotropy(), tio.RandomElasticDeformation()]),\n",
    ")\n",
    "transform = tio.Compose(transforms)\n",
    "transformed = transform(subject)\n",
    "reproduce_transform = transformed.get_composed_history()\n",
    "reproduce_transform  \n",
    "reproduced = reproduce_transform(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8bbc8",
   "metadata": {},
   "source": [
    "<img src='images/invertibility.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "# Mock a segmentation CNN\n",
    "def model(x):\n",
    "    return x\n",
    "subject = tio.datasets.Colin27()\n",
    "transform = tio.RandomAffine()\n",
    "segmentations = []\n",
    "num_segmentations = 10\n",
    "for _ in range(num_segmentations):\n",
    "    transform = tio.RandomAffine(image_interpolation='bspline')\n",
    "    transformed = transform(subject)\n",
    "    segmentation = model(transformed)\n",
    "    transformed_native_space = segmentation.apply_inverse_transform(image_interpolation='linear')\n",
    "    segmentations.append(transformed_native_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6de61",
   "metadata": {},
   "outputs": [],
   "source": [
    " => Transforms can be classified in three types, according to their degree of invertibility:\n",
    "\n",
    " * Lossless: transforms that can be inverted with no loss of information, such as RandomFlip, Pad, or RandomNoise.\n",
    "\n",
    " * Lossy: transforms that can be inverted with some loss of information, such as RandomAffine, or Crop.\n",
    "\n",
    " * Impossible: transforms that cannot be inverted, such as RandomBlur.\n",
    "\n",
    " * Non-invertible transforms will be ignored by the apply_inverse_transform() method of Subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4bc09",
   "metadata": {},
   "source": [
    "<img src='images/inter.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7266aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "transform = tio.RandomAffine(image_interpolation='bspline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c47b0",
   "metadata": {},
   "source": [
    "<img src='images/inter2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    " => There are 2 parts in Transform - Pre-processing and Augmentation .\n",
    "    \n",
    " => Refer to Transform and augmentations PDF file for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a2c45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
