{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3054827",
   "metadata": {},
   "source": [
    "#   Torch IO library for 3-D images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a41b7",
   "metadata": {},
   "source": [
    "##   TorchIO Image class\n",
    "\n",
    "> The Image class, representing one medical image, stores a 4D tensor, whose voxels encode, e.g., signal intensity or segmentation labels, and the corresponding affine transform, typically a rigid (Euclidean) transform, to convert voxel indices to world coordinates in mm. Arbitrary fields such as acquisition parameters may also be stored.\n",
    "\n",
    "> Subclasses are used to indicate specific types of images, such as ScalarImage and LabelMap, which are used to store, e.g., CT scans and segmentations, respectively.\n",
    "\n",
    "> An instance of Image can be created using a filepath, a PyTorch tensor, or a NumPy array. This class uses lazy loading, i.e., the data is not loaded from disk at instantiation time. Instead, the data is only loaded when needed for an operation (e.g., if a transform is applied to the image).\n",
    "\n",
    "> The figure below shows two instances of Image. The instance of ScalarImage contains a 4D tensor representing a diffusion MRI, which contains four 3D volumes (one per gradient direction), and the associated affine matrix. Additionally, it stores the strength and direction for each of the four gradients. The instance of LabelMap contains a brain parcellation of the same subject, the associated affine matrix, and the name and color of each brain structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb858c",
   "metadata": {},
   "source": [
    "<img src='https://torchio.readthedocs.io/_images/data_structures.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0beb32f1",
   "metadata": {},
   "source": [
    "##   class torchio.ScalarImage(*args, **kwargs)[source] =>  Image whose pixel values represent scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "# Loading from a file\n",
    "t1_image = tio.ScalarImage('t1.nii.gz')\n",
    "dmri = tio.ScalarImage(tensor=torch.rand(32, 128, 128, 88))\n",
    "image = tio.ScalarImage('safe_image.nrrd', check_nans=False)\n",
    "data, affine = image.data, image.affine\n",
    "\n",
    "print (affine.shape)\n",
    "print (image.data is image[tio.DATA])\n",
    "print (image.data is image.tensor )\n",
    "print (type(image.data) )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6110156f",
   "metadata": {},
   "source": [
    "##   class torchio.LabelMap(*args, **kwargs)[source]  =>  Image whose pixel values represent categorical labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b267dc0",
   "metadata": {},
   "source": [
    "> Intensity transforms are not applied to these images.\n",
    "\n",
    "> Nearest neighbor interpolation is always used to resample label maps, independently of the specified interpolation type in the transform instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770289da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "labels = tio.LabelMap(tensor=torch.rand(1, 128, 128, 68) > 0.5)\n",
    "labels = tio.LabelMap('t1_seg.nii.gz')  \n",
    "tpm = tio.LabelMap('gray_matter.nii.gz', 'white_matter.nii.gz', 'csf.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a822a",
   "metadata": {},
   "source": [
    "<img src='images/image_class.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1401a",
   "metadata": {},
   "source": [
    "<img src='images/parameters.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "001ceb2b",
   "metadata": {},
   "source": [
    "##  TorchIO images are lazy loaders, i.e. the data is only loaded from disk when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0126d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "import numpy as np\n",
    "\n",
    "image = tio.ScalarImage('t1.nii.gz')  # subclass of Image\n",
    "image  # not loaded yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa55c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_two = 2 * image.data  # data is loaded and cached here\n",
    "print (image)\n",
    "\n",
    "image.save('doubled_image.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function\n",
    "def numpy_reader(path):\n",
    "    data = np.load(path).as_type(np.float32)\n",
    "    affine = np.eye(4)\n",
    "    return data, affine\n",
    "\n",
    "image = tio.ScalarImage('t1.npy', reader=numpy_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d1902",
   "metadata": {},
   "outputs": [],
   "source": [
    " => affine: numpy.ndarray => Affine matrix to transform voxel indices into world coordinates.\n",
    "        \n",
    " => as_pil(transpose=True) => Get the image as an instance of PIL.Image.\n",
    "\n",
    "     ** Note => Values will be clamped to 0-255 and cast to uint8 and \n",
    "                    To use this method, Pillow needs to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efb1af",
   "metadata": {},
   "outputs": [],
   "source": [
    " => as_sitk(**kwargs) → SimpleITK.SimpleITK.Image  => Get the image as an instance of sitk.Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a93c67",
   "metadata": {},
   "outputs": [],
   "source": [
    " => axis_name_to_index(axis: str) => int  =>  Convert an axis name to an axis index.\n",
    "    \n",
    "   ** axis – Possible inputs are 'Left', 'Right', 'Anterior', 'Posterior', 'Inferior', 'Superior'. \n",
    "      \n",
    "   ** Lower-case versions and first letters are also valid, as only the first letter will be used.\n",
    "\n",
    " => If you are working with animals, you should probably use 'Superior', 'Inferior', 'Anterior' and 'Posterior'\n",
    "     for 'Dorsal', 'Ventral', 'Rostral' and 'Caudal', respectively.\n",
    "    \n",
    " => If your images are 2D, you can use 'Top', 'Bottom', 'Left' and 'Right'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e428b",
   "metadata": {},
   "outputs": [],
   "source": [
    " => bounds: numpy.ndarray  =>  Position of centers of voxels in smallest and largest indices.\n",
    "\n",
    " => data: torch.Tensor  =>  Tensor data (Same as Image.tensor)\n",
    "    \n",
    " => static flip_axis(axis: str) → str[source]  => Return the opposite axis label. For example, 'L' -> 'R'.\n",
    "\n",
    "  ** axis – Axis label, such as 'L' or 'left'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  classmethod from_sitk(sitk_image)  =>  Instantiate a new TorchIO image from a sitk.Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "import SimpleITK as sitk\n",
    "\n",
    "sitk_image = sitk.Image(20, 30, 40, sitk.sitkUInt16)\n",
    "tio.LabelMap.from_sitk(sitk_image)\n",
    "sitk_image = sitk.Image((224, 224), sitk.sitkVectorFloat32, 3)\n",
    "tio.ScalarImage.from_sitk(sitk_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08042d",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  get_bounds() => Tuple[Tuple[float, float], Tuple[float, float], Tuple[float, float]]\n",
    "                          Get minimum and maximum world coordinates occupied by the image.\n",
    "        \n",
    " =>  get_center(lps: bool = False) => Tuple[float, float, float] =>  Get image center in RAS+ or LPS+ coordinates.\n",
    "\n",
    "    ** lps – If True, the coordinates will be in LPS+ orientation, i.e. the first dimension grows towards\n",
    "               the left etc. Otherwise, the coordinates will be in RAS+ orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae17d87",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  height: int  =>  Image height, if 2D.\n",
    "        \n",
    " =>  itemsize =>  Element size of the data type.\n",
    "\n",
    " =>  load() => None => Load the image from disk.\n",
    "    \n",
    "     Returns => Tuple containing a 4D tensor of size (C,W,H,D) and a 2D(4,4)  affine matrix to convert \n",
    "                 voxel indices to world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  memory: float  =>  Number of Bytes that the tensor takes in the RAM.\n",
    "        \n",
    " =>  num_channels: int  => Get the number of channels in the associated 4D tensor.\n",
    "\n",
    " =>  numpy() → numpy.ndarray  =>  Get a NumPy array containing the image data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3114d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    " => orientation: Tuple[str, str, str]  =>  Orientation codes.\n",
    "        \n",
    " => origin: Tuple[float, float, float]  => Center of first voxel in array, in mm.\n",
    "\n",
    " => plot(**kwargs) → None =>  Plot image.\n",
    "\n",
    " => save(path: Union[str, os.PathLike], squeeze: Optional[bool] = None) => None  =>  Save image to disk.\n",
    "\n",
    "      ** path – String or instance of pathlib.Path.\n",
    "\n",
    "      ** squeeze – Whether to remove singleton dimensions before saving. \n",
    "                     If None, the array will be squeezed if the output format is JP(E)G, PNG, BMP or TIF(F).\n",
    "\n",
    "\n",
    " => set_data(tensor: Union[torch.Tensor, numpy.ndarray]) =>  Store a 4D tensor in the data key and attribute.\n",
    "\n",
    "     ** tensor – 4D tensor (C,W,H,D) with dimensions .\n",
    "\n",
    " => shape: Tuple[int, int, int, int] => Tensor shape as (C,W,H,D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d688a",
   "metadata": {},
   "outputs": [],
   "source": [
    " => show(viewer_path: Optional[Union[str, os.PathLike]] = None) => None => Open the image using external software.\n",
    "    \n",
    "    **  viewer_path – Path to the application used to view the image. \n",
    "                      If None, the value of the environment variable SITK_SHOW_COMMAND will be used. \n",
    "                      If this variable is also not set, TorchIO will try to guess the location of ITK-SNAP \n",
    "                         and 3D Slicer.  RAISES RuntimeError – If the viewer is not found.\n",
    " => spacing: Tuple[float, float, float] => Voxel spacing in mm.\n",
    "    \n",
    " => spatial_shape: Tuple[int, int, int]  =>  Tensor spatial shape as (C,W,H,D) .\n",
    "    \n",
    " => tensor: torch.Tensor =>  Tensor data (Same as Image.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d63d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    " => to_gif(axis: int, duration: float, output_path: Union[str, os.PathLike], loop: int = 0,\n",
    "            rescale: bool = True, optimize: bool = True, reverse: bool = False) => None\n",
    "    \n",
    " => to_gif() saves an animated GIF of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a5828",
   "metadata": {},
   "source": [
    "<img src=\"images/to_gif.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582023dd",
   "metadata": {},
   "source": [
    " ##  Subject Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb13f4",
   "metadata": {},
   "source": [
    "<img src='images/subject_class.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "\n",
    "# One way:\n",
    "subject = tio.Subject(one_image=tio.ScalarImage('path_to_image.nii.gz'), \n",
    "                      a_segmentation=tio.LabelMap('path_to_seg.nii.gz'),\n",
    "                      age=45, name='John Doe', hospital='Hospital Juan Negrín')\n",
    "\n",
    "# If you want to create the mapping before, or have spaces in the keys -\n",
    "subject_dict = {'one image': tio.ScalarImage('path_to_image.nii.gz'),\n",
    "                'a segmentation': tio.LabelMap('path_to_seg.nii.gz'),\n",
    "                'age': 45, 'name': 'John Doe', 'hospital': 'Hospital Juan Negrín'}\n",
    "\n",
    "subject = tio.Subject(subject_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd513f",
   "metadata": {},
   "outputs": [],
   "source": [
    " => add_image(image: torchio.data.image.Image, image_name: str) => None  =>  Add an image.\n",
    "\n",
    " => apply_inverse_transform(**kwargs) → torchio.data.subject.Subject => Try to apply the inverse of all \n",
    "                                                                          applied transforms, in reverse order.\n",
    "\n",
    "       **kwargs – Keyword arguments passed on to get_inverse_transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cbf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "##   Check for consistency of an attribute across all images.\n",
    "\n",
    "=> check_consistent_attribute(attribute: str, relative_tolerance: float = 1e-06, \n",
    "                               absolute_tolerance: float = 1e-06, message: Optional[str] = None) → None\n",
    "    \n",
    "  ## Parameters  -\n",
    "   \n",
    "   * attribute – Name of the image attribute to check\n",
    "\n",
    "   * relative_tolerance – Relative tolerance for numpy.allclose()\n",
    "\n",
    "   * absolute_tolerance – Absolute tolerance for numpy.allclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "scalars = torch.randn(1, 512, 512, 100)\n",
    "mask = torch.tensor(scalars > 0).type(torch.int16)\n",
    "\n",
    "af1 = np.eye([0.8, 0.8, 2.50000000000001, 1])\n",
    "af2 = np.eye([0.8, 0.8, 2.49999999999999, 1])  # small difference here (e.g. due to different reader)\n",
    "\n",
    "subject = tio.Subject(image = tio.ScalarImage(tensor=scalars, affine=af1),\n",
    "                      mask = tio.LabelMap(tensor=mask, affine=af2))\n",
    "\n",
    "subject.check_consistent_attribute('spacing')  # no error as tolerances are > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5463545",
   "metadata": {},
   "source": [
    "<img src='images/relative_tolerance.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec797a2",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Get a reversed list of the inverses of the applied transforms.\n",
    "    \n",
    "=> get_inverse_transform(warn: bool = True, ignore_intensity: bool = True, image_interpolation: \n",
    "                           Optional[str] = None) → Compose \n",
    "    \n",
    " \n",
    "          Parameters \n",
    "    \n",
    "    * warn – Issue a warning if some transforms are not invertible.\n",
    "\n",
    "    * ignore_intensity – If True, all instances of IntensityTransform will be ignored.\n",
    "\n",
    "    * image_interpolation – Modify interpolation for scalar images inside transforms that perform resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ba941",
   "metadata": {},
   "outputs": [],
   "source": [
    " => load() → None  =>  Load images in subject on RAM.\n",
    "    \n",
    " => plot(**kwargs) → None  =>  Plot images using matplotlib.\n",
    "\n",
    "        **kwargs – Keyword arguments that will be passed on to plot()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8556ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    " => remove_image(image_name: str) => None  =>  Remove an image.\n",
    "    \n",
    " => shape => Return shape of first image in subject.\n",
    "\n",
    " ** Note => Consistency of shapes across images in the subject is checked first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "\n",
    "colin = tio.datasets.Colin27()\n",
    "colin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96347d96",
   "metadata": {},
   "outputs": [],
   "source": [
    " => spacing => Return spacing of first image in subject.\n",
    "\n",
    "  ** Note =>  Consistency of spacings across images in the subject is checked first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33165ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "\n",
    "colin = tio.datasets.Slicer()\n",
    "colin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    " => spatial_shape  =>  Return spatial shape of first image in subject.\n",
    "\n",
    "  ** Note =>  Consistency of spatial shapes across images in the subject is checked first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "\n",
    "colin = tio.datasets.Colin27()\n",
    "colin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ffc5a4",
   "metadata": {},
   "source": [
    "##  DATASET Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f5915",
   "metadata": {},
   "source": [
    "<img src='https://torchio.readthedocs.io/_images/diagram_volumes.svg' />\n",
    "</br>\n",
    "<img src='images/dataset.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "subject_a = tio.Subject(\n",
    "    t1=tio.ScalarImage('t1.nrrd',),\n",
    "    t2=tio.ScalarImage('t2.mha',),\n",
    "    label=tio.LabelMap('t1_seg.nii.gz'),\n",
    "    age=31,\n",
    "    name='Fernando Perez',\n",
    ")\n",
    "subject_b = tio.Subject(\n",
    "    t1=tio.ScalarImage('colin27_t1_tal_lin.minc',),\n",
    "    t2=tio.ScalarImage('colin27_t2_tal_lin_dicom',),\n",
    "    label=tio.LabelMap('colin27_seg1.nii.gz'),\n",
    "    age=56,\n",
    "    name='Colin Holmes',\n",
    ")\n",
    "subjects_list = [subject_a, subject_b]\n",
    "transforms = [\n",
    "    tio.RescaleIntensity(out_min_max=(0, 1)),\n",
    "    tio.RandomAffine(),\n",
    "]\n",
    "transform = tio.Compose(transforms)\n",
    "subjects_dataset = tio.SubjectsDataset(subjects_list, transform=transform)\n",
    "subject = subjects_dataset[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e126a4",
   "metadata": {},
   "outputs": [],
   "source": [
    " => To quickly iterate over the subjects without loading the images, use dry_iter().\n",
    "\n",
    " =>  dry_iter()  =>  Return the internal list of subjects.\n",
    "\n",
    " => This can be used to iterate over the subjects without loading the data and applying any transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [subject.name for subject in dataset.dry_iter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e941c",
   "metadata": {},
   "outputs": [],
   "source": [
    " => classmethod from_batch(batch: Dict) → torchio.data.dataset.SubjectsDataset\n",
    "    \n",
    " => Instantiates a dataset from a batch generated by a data loader.\n",
    "\n",
    "    ** batch – Dictionary generated by a data loader, containing data that can be converted to \n",
    "                  instances of Subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  set_transform(transform: Optional[Callable]) =>  None  =>  Set the transform attribute\n",
    "    \n",
    "    ** transform – Callable object, typically an subclass of torchio.transforms.Transform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
